positive = "1")
caret::confusionMatrix(testProbs$Eng_predOutcome, testProbs$Outcome,
positive = "1")
pROC::auc(testProbs$Outcome, testProbs$KS_Probs)
pROC::auc(testProbs$Outcome, testProbs$Eng_Probs)
# split into 65/35 train/test
set.seed(3456)
trainIndex <- createDataPartition(homeowners$y, p = .65,
list = FALSE,
times = 1)
homeownersTrain <- homeowners[ trainIndex,]
homeownersTest  <- homeowners[-trainIndex,]
# kitchen sink model
kitchensink_reg <- glm(y_numeric ~ ., data = homeownersTrain %>% select(-y, -prev_success, -ed_cat, -is_single, -high_unemp,
-high_repair_spending, -high_inflation, -big_month),
family = "binomial" (link = "logit"))
# model w feature engineering
engineered_reg <- glm(y_numeric ~ ., data = homeownersTrain %>% select(-y, -marital, -education, -inflation_rate,
-spent_on_repairs, -poutcome, -cons.conf.idx,
-pdays, -taxLien, -age, -mortgage, -taxbill_in_phl,
-day_of_week, -previous, -ed_cat, -is_single, -high_unemp,
-high_repair_spending, -high_inflation, -month, -job))
summary(engineered_reg)
# quick sensitivity code for engineering optimization (delete when done)
testProbs <- data.frame(Outcome = as.factor(homeownersTest$y_numeric),
KS_Probs = predict(kitchensink_reg, homeownersTest, type = "response"),
Eng_Probs = predict(engineered_reg, homeownersTest, type = "response"))
ggplot(testProbs %>% pivot_longer(-Outcome), aes(x = value, fill = as.factor(Outcome))) +
geom_density() +
facet_grid(Outcome ~ name) +
scale_fill_manual(values = palette2) + xlim(0, 1) +
labs(x = "Took Housing Subsidy", y = "Density of probabilities",
title = "Distribution of predicted probabilities by observed outcome") +
plotTheme() + theme(strip.text.x = element_text(size = 18),
legend.position = "none")
testProbs <-
testProbs %>%
mutate(KS_predOutcome  = as.factor(ifelse(testProbs$KS_Probs > 0.15, 1, 0)),
Eng_predOutcome = as.factor(ifelse(testProbs$Eng_Probs > 0.25, 1, 0)))
caret::confusionMatrix(testProbs$KS_predOutcome, testProbs$Outcome,
positive = "1")
caret::confusionMatrix(testProbs$Eng_predOutcome, testProbs$Outcome,
positive = "1")
pROC::auc(testProbs$Outcome, testProbs$KS_Probs)
pROC::auc(testProbs$Outcome, testProbs$Eng_Probs)
# quick sensitivity code for engineering optimization (delete when done)
testProbs <- data.frame(Outcome = as.factor(homeownersTest$y_numeric),
KS_Probs = predict(kitchensink_reg, homeownersTest, type = "response"),
Eng_Probs = predict(engineered_reg, homeownersTest, type = "response"))
ggplot(testProbs %>% pivot_longer(-Outcome), aes(x = value, fill = as.factor(Outcome))) +
geom_density() +
facet_grid(Outcome ~ name) +
scale_fill_manual(values = palette2) + xlim(0, 1) +
labs(x = "Took Housing Subsidy", y = "Density of probabilities",
title = "Distribution of predicted probabilities by observed outcome") +
plotTheme() + theme(strip.text.x = element_text(size = 18),
legend.position = "none")
testProbs <-
testProbs %>%
mutate(KS_predOutcome  = as.factor(ifelse(testProbs$KS_Probs > 0.15, 1, 0)),
Eng_predOutcome = as.factor(ifelse(testProbs$Eng_Probs > 0.2, 1, 0)))
caret::confusionMatrix(testProbs$KS_predOutcome, testProbs$Outcome,
positive = "1")
caret::confusionMatrix(testProbs$Eng_predOutcome, testProbs$Outcome,
positive = "1")
pROC::auc(testProbs$Outcome, testProbs$KS_Probs)
pROC::auc(testProbs$Outcome, testProbs$Eng_Probs)
# split into 65/35 train/test
set.seed(3456)
trainIndex <- createDataPartition(homeowners$y, p = .65,
list = FALSE,
times = 1)
homeownersTrain <- homeowners[ trainIndex,]
homeownersTest  <- homeowners[-trainIndex,]
# kitchen sink model
kitchensink_reg <- glm(y_numeric ~ ., data = homeownersTrain %>% select(-y, -prev_success, -ed_cat, -is_single, -high_unemp,
-high_repair_spending, -high_inflation, -big_month),
family = "binomial" (link = "logit"))
# model w feature engineering
engineered_reg <- glm(y_numeric ~ ., data = homeownersTrain %>% select(-y, -marital, -education, -inflation_rate,
-spent_on_repairs, -poutcome, -cons.conf.idx,
-pdays, -taxLien, -age, -mortgage, -taxbill_in_phl,
-day_of_week, -previous, -ed_cat, -is_single, -high_unemp,
-high_repair_spending, -high_inflation, -month, -job,
-campaign))
summary(engineered_reg)
# quick sensitivity code for engineering optimization (delete when done)
testProbs <- data.frame(Outcome = as.factor(homeownersTest$y_numeric),
KS_Probs = predict(kitchensink_reg, homeownersTest, type = "response"),
Eng_Probs = predict(engineered_reg, homeownersTest, type = "response"))
ggplot(testProbs %>% pivot_longer(-Outcome), aes(x = value, fill = as.factor(Outcome))) +
geom_density() +
facet_grid(Outcome ~ name) +
scale_fill_manual(values = palette2) + xlim(0, 1) +
labs(x = "Took Housing Subsidy", y = "Density of probabilities",
title = "Distribution of predicted probabilities by observed outcome") +
plotTheme() + theme(strip.text.x = element_text(size = 18),
legend.position = "none")
testProbs <-
testProbs %>%
mutate(KS_predOutcome  = as.factor(ifelse(testProbs$KS_Probs > 0.15, 1, 0)),
Eng_predOutcome = as.factor(ifelse(testProbs$Eng_Probs > 0.2, 1, 0)))
caret::confusionMatrix(testProbs$KS_predOutcome, testProbs$Outcome,
positive = "1")
caret::confusionMatrix(testProbs$Eng_predOutcome, testProbs$Outcome,
positive = "1")
pROC::auc(testProbs$Outcome, testProbs$KS_Probs)
pROC::auc(testProbs$Outcome, testProbs$Eng_Probs)
# split into 65/35 train/test
set.seed(3456)
trainIndex <- createDataPartition(homeowners$y, p = .65,
list = FALSE,
times = 1)
homeownersTrain <- homeowners[ trainIndex,]
homeownersTest  <- homeowners[-trainIndex,]
# kitchen sink model
kitchensink_reg <- glm(y_numeric ~ ., data = homeownersTrain %>% select(-y, -prev_success, -ed_cat, -is_single, -high_unemp,
-high_repair_spending, -high_inflation, -big_month),
family = "binomial" (link = "logit"))
# model w feature engineering
engineered_reg <- glm(y_numeric ~ ., data = homeownersTrain %>% select(-y, -marital, -education, -inflation_rate,
-spent_on_repairs, -poutcome, -cons.conf.idx,
-pdays, -taxLien, -mortgage, -taxbill_in_phl,
-day_of_week, -previous, -ed_cat, -is_single, -high_unemp,
-high_repair_spending, -high_inflation, -month, -job,
-campaign))
summary(engineered_reg)
# quick sensitivity code for engineering optimization (delete when done)
testProbs <- data.frame(Outcome = as.factor(homeownersTest$y_numeric),
KS_Probs = predict(kitchensink_reg, homeownersTest, type = "response"),
Eng_Probs = predict(engineered_reg, homeownersTest, type = "response"))
ggplot(testProbs %>% pivot_longer(-Outcome), aes(x = value, fill = as.factor(Outcome))) +
geom_density() +
facet_grid(Outcome ~ name) +
scale_fill_manual(values = palette2) + xlim(0, 1) +
labs(x = "Took Housing Subsidy", y = "Density of probabilities",
title = "Distribution of predicted probabilities by observed outcome") +
plotTheme() + theme(strip.text.x = element_text(size = 18),
legend.position = "none")
testProbs <-
testProbs %>%
mutate(KS_predOutcome  = as.factor(ifelse(testProbs$KS_Probs > 0.15, 1, 0)),
Eng_predOutcome = as.factor(ifelse(testProbs$Eng_Probs > 0.2, 1, 0)))
caret::confusionMatrix(testProbs$KS_predOutcome, testProbs$Outcome,
positive = "1")
caret::confusionMatrix(testProbs$Eng_predOutcome, testProbs$Outcome,
positive = "1")
pROC::auc(testProbs$Outcome, testProbs$KS_Probs)
pROC::auc(testProbs$Outcome, testProbs$Eng_Probs)
library(lubridate)
library(tidyverse)
library(caret)
library(kableExtra)
library(ModelMetrics)
library(plotROC)
library(knitr)
library(grid)
library(gridExtra)
library(QuantPsyc)
install.packages("QuantPsyc")
knitr::opts_chunk$set(echo = TRUE)
library(lubridate)
library(tidyverse)
library(caret)
library(kableExtra)
library(ModelMetrics)
library(plotROC)
library(knitr)
library(grid)
library(gridExtra)
library(QuantPsyc)
root.dir = "https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/DATA/"
source("https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/functions.r")
palette_9_colors <- c("#FF2AD4","#E53AD8","#CC4ADC","#996AE5","#7F7BE9",
"#668BED","#33ABF6","#19BBFA","#00CCFF")
palette_3_colors <- c("#FF2AD4","#7F7BE9","#00CCFF")
palette_2_colors <- c("#FF2AD4", "#00CCFF")
palette_1_colors <- c("#00CCFF")
# feature engineering
raw_data <- read.csv(file.path(root.dir,"Chapter7/compas-scores-two-years.csv"))
df <-
raw_data %>%
filter(days_b_screening_arrest <= 30) %>%
filter(days_b_screening_arrest >= -30) %>%
filter(is_recid != -1) %>%
filter(c_charge_degree != "O") %>%
filter(priors_count != "36") %>%
filter(priors_count != "25") %>%
mutate(length_of_stay = as.numeric(as.Date(c_jail_out) - as.Date(c_jail_in)),
priors_count = as.factor(priors_count),
Recidivated = as.factor(ifelse(two_year_recid == 1,"Recidivate","notRecidivate")),
recidivatedNumeric = ifelse(Recidivated == "Recidivate", 1, 0),
race2 = case_when(race == "Caucasian"        ~ "Caucasian",
race == "African-American" ~ "African-American",
TRUE                       ~ "Other")) %>%
dplyr::select(sex,age,age_cat,race,race2,priors_count,two_year_recid,r_charge_desc,
c_charge_desc,c_charge_degree,r_charge_degree,juv_other_count,
length_of_stay,priors_count,Recidivated,recidivatedNumeric) %>%
filter(priors_count != 38)
# modeling setup
train <- df %>% dplyr::sample_frac(.75)
train_index <- as.numeric(rownames(train))
test <- df[-train_index, ]
# model definition
reg.noRace <- glm(Recidivated ~ ., data =
train %>% dplyr::select(sex, age, age_cat,
juv_other_count, length_of_stay,
priors_count, Recidivated),
family = "binomial"(link = "logit"))
# testing
testProbs <-
data.frame(class = test$recidivatedNumeric,
probs = predict(reg.noRace, test, type = "response"),
Race = test$race2)
# confusion metrics by race
iterateThresholds <- function(data, observedClass, predictedProbs, group) {
observedClass <- enquo(observedClass)
predictedProbs <- enquo(predictedProbs)
group <- enquo(group)
x = .01
all_prediction <- data.frame()
if (missing(group)) {
while (x <= 1) {
this_prediction <- data.frame()
this_prediction <-
data %>%
mutate(predclass = ifelse(!!predictedProbs > x, 1,0)) %>%
count(predclass, !!observedClass) %>%
summarize(Count_TN = sum(n[predclass==0 & !!observedClass==0]),
Count_TP = sum(n[predclass==1 & !!observedClass==1]),
Count_FN = sum(n[predclass==0 & !!observedClass==1]),
Count_FP = sum(n[predclass==1 & !!observedClass==0]),
Rate_TP = Count_TP / (Count_TP + Count_FN),
Rate_FP = Count_FP / (Count_FP + Count_TN),
Rate_FN = Count_FN / (Count_FN + Count_TP),
Rate_TN = Count_TN / (Count_TN + Count_FP),
Accuracy = (Count_TP + Count_TN) /
(Count_TP + Count_TN + Count_FN + Count_FP)) %>%
mutate(Threshold = round(x,2))
all_prediction <- rbind(all_prediction,this_prediction)
x <- x + .01
}
return(all_prediction)
}
else if (!missing(group)) {
while (x <= 1) {
this_prediction <- data.frame()
this_prediction <-
data %>%
mutate(predclass = ifelse(!!predictedProbs > x, 1,0)) %>%
group_by(!!group) %>%
count(predclass, !!observedClass) %>%
summarize(Count_TN = sum(n[predclass==0 & !!observedClass==0]),
Count_TP = sum(n[predclass==1 & !!observedClass==1]),
Count_FN = sum(n[predclass==0 & !!observedClass==1]),
Count_FP = sum(n[predclass==1 & !!observedClass==0]),
Rate_TP = Count_TP / (Count_TP + Count_FN),
Rate_FP = Count_FP / (Count_FP + Count_TN),
Rate_FN = Count_FN / (Count_FN + Count_TP),
Rate_TN = Count_TN / (Count_TN + Count_FP),
Accuracy = (Count_TP + Count_TN) /
(Count_TP + Count_TN + Count_FN + Count_FP)) %>%
mutate(Threshold = round(x, 2))
all_prediction <- rbind(all_prediction, this_prediction)
x <- x + .01
}
return(all_prediction)
}
}
# confusion metrics by race
iterateThresholds <- function(data, observedClass, predictedProbs, group) {
observedClass <- enquo(observedClass)
predictedProbs <- enquo(predictedProbs)
group <- enquo(group)
x = .01
all_prediction <- data.frame()
if (missing(group)) {
while (x <= 1) {
this_prediction <- data.frame()
this_prediction <-
data %>%
mutate(predclass = ifelse(!!predictedProbs > x, 1,0)) %>%
count(predclass, !!observedClass) %>%
summarize(Count_TN = sum(n[predclass==0 & !!observedClass==0]),
Count_TP = sum(n[predclass==1 & !!observedClass==1]),
Count_FN = sum(n[predclass==0 & !!observedClass==1]),
Count_FP = sum(n[predclass==1 & !!observedClass==0]),
Rate_TP = Count_TP / (Count_TP + Count_FN),
Rate_FP = Count_FP / (Count_FP + Count_TN),
Rate_FN = Count_FN / (Count_FN + Count_TP),
Rate_TN = Count_TN / (Count_TN + Count_FP),
Accuracy = (Count_TP + Count_TN) /
(Count_TP + Count_TN + Count_FN + Count_FP)) %>%
mutate(Threshold = round(x,2))
all_prediction <- rbind(all_prediction,this_prediction)
x <- x + .01
}
return(all_prediction)
}
else if (!missing(group)) {
while (x <= 1) {
this_prediction <- data.frame()
this_prediction <-
data %>%
mutate(predclass = ifelse(!!predictedProbs > x, 1,0)) %>%
group_by(!!group) %>%
count(predclass, !!observedClass) %>%
summarize(Count_TN = sum(n[predclass==0 & !!observedClass==0]),
Count_TP = sum(n[predclass==1 & !!observedClass==1]),
Count_FN = sum(n[predclass==0 & !!observedClass==1]),
Count_FP = sum(n[predclass==1 & !!observedClass==0]),
Rate_TP = Count_TP / (Count_TP + Count_FN),
Rate_FP = Count_FP / (Count_FP + Count_TN),
Rate_FN = Count_FN / (Count_FN + Count_TP),
Rate_TN = Count_TN / (Count_TN + Count_FP),
Accuracy = (Count_TP + Count_TN) /
(Count_TP + Count_TN + Count_FN + Count_FP)) %>%
mutate(Threshold = round(x, 2))
all_prediction <- rbind(all_prediction, this_prediction)
x <- x + .01
}
return(all_prediction)
}
}
testProbs.thresholds <-
iterateThresholds(data=testProbs, observedClass = class,
predictedProbs = probs, group = Race)
filter(testProbs.thresholds, Threshold == .5)  %>%
dplyr::select(Accuracy, Race, starts_with("Rate")) %>%
gather(Variable, Value, -Race) %>%
ggplot(aes(Variable, Value, fill = Race)) +
geom_bar(aes(fill = Race), position = "dodge", stat = "identity") +
scale_fill_manual(values = palette_3_colors) +
labs(title="Confusion matrix rates by race",
subtitle = "50% threshold", x = "Outcome",y = "Rate") +
plotTheme() + theme(axis.text.x = element_text(angle = 45, hjust = 1))
View(testProbs.thresholds)
# cost-benefit setup
testProbs.thresholds <-
testProbs.thresholds %>%
dplyr::select(starts_with("Count"), Threshold, Race) %>%
gather(Variable, Count, -Threshold, -Race) %>%
mutate(Revenue =
case_when(Variable == "Count_TN"  ~ 0,
Variable == "Count_TP"  ~ 4830 * Count,
Variable == "Count_FN"  ~ 0,
Variable == "Count_FP"  ~ (-2850) * Count))
View(testProbs.thresholds)
testProbs.thresholds <-
iterateThresholds(data=testProbs, observedClass = class,
predictedProbs = probs, group = Race)
# cost-benefit setup
# TODO: replace cost/benefit numbers based on research/common sense
# TODO: make sure to bake in rate of program success into cost/benefit
testProbs.thresholds_revenue <-
testProbs.thresholds %>%
dplyr::select(starts_with("Count"), Threshold, Race) %>%
gather(Variable, Count, -Threshold, -Race) %>%
mutate(Revenue =
case_when(Variable == "Count_TN"  ~ 0,
Variable == "Count_TP"  ~ 4830 * Count,
Variable == "Count_FN"  ~ 0,
Variable == "Count_FP"  ~ (-2850) * Count))
# cost-benefit setup
# TODO: replace cost/benefit numbers based on research/common sense
# TODO: make sure to bake in rate of program success into cost/benefit
testProbs.thresholds_revenue <-
testProbs.thresholds %>%
dplyr::select(starts_with("Count"), Threshold, Race) %>%
gather(Variable, Count, -Threshold, -Race) %>%
mutate(Revenue =
case_when(Variable == "Count_TN"  ~ 0,
Variable == "Count_TP"  ~ 4830 * Count,
Variable == "Count_FN"  ~ 0,
Variable == "Count_FP"  ~ (-2850) * Count)) %>%
group_by(Threshold) %>%
summarize(Revenue = sum(Revenue))
View(testProbs.thresholds_revenue)
# cost-benefit setup
# TODO: replace cost/benefit numbers based on research/common sense
# TODO: make sure to bake in rate of program success into cost/benefit
testProbs.thresholds_revenue <-
testProbs.thresholds %>%
dplyr::select(starts_with("Count"), Threshold, Race) %>%
gather(Variable, Count, -Threshold, -Race) %>%
mutate(Revenue =
case_when(Variable == "Count_TN"  ~ 0,
Variable == "Count_TP"  ~ 4830 * Count,
Variable == "Count_FN"  ~ 0,
Variable == "Count_FP"  ~ (-2850) * Count)) %>%
group_by(Threshold, Race) %>%
summarize(Revenue = sum(Revenue))
View(testProbs.thresholds_revenue)
View(testProbs.thresholds_revenue)
View(testProbs.thresholds_revenue)
# plotting revenue by threshold by race (probably do include in final to explain equitable threshold?)
whichThreshold_revenue %>%
ggplot(aes(Threshold, Revenue)) +
geom_point(color = Race) +
geom_vline(xintercept = pull(arrange(whichThreshold_revenue, -Revenue)[1,1]), color = "red") +
scale_colour_manual(values = palette2) +
plotTheme() +
labs(title = "Revenue and number of credits allocated per threshold",
subtitle = "Red line denotes optimal threshold")
# plotting revenue by threshold by race (probably do include in final to explain equitable threshold?)
testProbs.thresholds_revenue %>%
ggplot(aes(Threshold, Revenue)) +
geom_point(color = Race) +
geom_vline(xintercept = pull(arrange(whichThreshold_revenue, -Revenue)[1,1]), color = "red") +
scale_colour_manual(values = palette2) +
plotTheme() +
labs(title = "Revenue and number of credits allocated per threshold",
subtitle = "Red line denotes optimal threshold")
# plotting revenue by threshold by race (probably do include in final to explain equitable threshold?)
testProbs.thresholds_revenue %>%
ggplot() +
geom_point(aes(Threshold, Revenue, color = Race)) +
geom_vline(xintercept = pull(arrange(whichThreshold_revenue, -Revenue)[1,1]), color = "red") +
scale_colour_manual(values = palette2) +
plotTheme() +
labs(title = "Revenue and number of credits allocated per threshold",
subtitle = "Red line denotes optimal threshold")
# plotting revenue by threshold by race (probably do include in final to explain equitable threshold?)
testProbs.thresholds_revenue %>%
ggplot() +
geom_point(aes(Threshold, Revenue, color = Race)) +
geom_vline(xintercept = pull(arrange(testProbs.thresholds_revenue, -Revenue)[1,1]), color = "red") +
scale_colour_manual(values = palette2) +
plotTheme() +
labs(title = "Revenue and number of credits allocated per threshold",
subtitle = "Red line denotes optimal threshold")
# plotting revenue by threshold by race (probably do include in final to explain equitable threshold?)
testProbs.thresholds_revenue %>%
ggplot() +
geom_point(aes(Threshold, Revenue, color = Race)) +
geom_vline(xintercept = pull(arrange(testProbs.thresholds_revenue, -Revenue)[1,1]), color = "red") +
scale_colour_manual(values = palette_2_colors) +
plotTheme() +
labs(title = "Revenue and number of credits allocated per threshold",
subtitle = "Red line denotes optimal threshold")
# plotting revenue by threshold by race (probably do include in final to explain equitable threshold?)
testProbs.thresholds_revenue %>%
ggplot() +
geom_point(aes(Threshold, Revenue, color = Race)) +
geom_vline(xintercept = pull(arrange(testProbs.thresholds_revenue, -Revenue)[1,1]), color = "red") +
scale_colour_manual(values = palette_3_colors) +
plotTheme() +
labs(title = "Revenue and number of credits allocated per threshold",
subtitle = "Red line denotes optimal threshold")
thresh_50 <- filter(testProbs.thresholds, Threshold == .5)  %>%
dplyr::select(Accuracy, Race, starts_with("Rate")) %>%
gather(Variable, Value, -Race) %>%
ggplot(aes(Variable, Value, fill = Race)) +
geom_bar(aes(fill = Race), position = "dodge", stat = "identity") +
scale_fill_manual(values = palette_3_colors) +
labs(title="Confusion matrix rates by race",
subtitle = "50% threshold", x = "Outcome",y = "Rate") +
plotTheme() + theme(axis.text.x = element_text(angle = 45, hjust = 1))
# TODO: set up this graph with new equitable threshold based on above
thresh_equitable <- filter(testProbs.thresholds, Threshold == .3)  %>%
dplyr::select(Accuracy, Race, starts_with("Rate")) %>%
gather(Variable, Value, -Race) %>%
ggplot(aes(Variable, Value, fill = Race)) +
geom_bar(aes(fill = Race), position = "dodge", stat = "identity") +
scale_fill_manual(values = palette_3_colors) +
labs(title="Confusion matrix rates by race",
subtitle = "50% threshold", x = "Outcome",y = "Rate") +
plotTheme() + theme(axis.text.x = element_text(angle = 45, hjust = 1))
grid.arrange(thresh_50, thresh_equitable)
?grid.arrange
grid.arrange(thresh_50, thresh_equitable, ncol = 2)
library(tidyverse)
getwd()
setwd( "C:/Users/henry/Documents/HFdocs/Penn/Spring_2023/MUSA7900_Practicum/musa-rats/")
library(R.utils)
install.packages("R.utils")
library(R.utils)
data <- data.table("data/rats_to_blocks.csv.gz")
data <- read.table(gzfile("data/rats_to_blocks.csv.gz"))
data <- read_csv(gzfile("data/rats_to_blocks.csv.gz"))
View(data)
options(scipen=999)
options(scipen=999)
View(data)
# Import libraries
library(tidyverse)
library(sf)
## Rats to blocks
Rats2blocks <- read.csv("./notebooks/data/rats_to_blocks.csv.gz", header = TRUE) %>%
na.omit() %>%
st_as_sf(.,coords=c("LATITUDE","LONGITUDE"),crs=102685)
# Temp
source("https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/functions.r")
root.dir = "https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/DATA/"
# Etc
options(scipen=999)
## Rats to blocks
Rats2blocks <- read.csv("./notebooks/data/rats_to_blocks.csv.gz", header = TRUE) %>%
na.omit() %>%
st_as_sf(.,coords=c("LATITUDE","LONGITUDE"),crs=102685)
## Ward
Ward <- st_read("./notebooks/data/Wards.geojson") %>%
st_transform('ESRI:102685')
## Rats to blocks
Rats2blocks <- read.csv("https://github.com/mafichman/DOH-RodentAbatement-Public/tree/master/notebooks/data/notebooks/data/rats_to_blocks.csv.gz", header = TRUE) %>%
na.omit() %>%
st_as_sf(.,coords=c("LATITUDE","LONGITUDE"),crs=102685)
Rats2blocks <- read.csv("https://github.com/mafichman/DOH-RodentAbatement-Public/tree/master/notebooks/data/rats_to_blocks.csv.gz", header = TRUE) %>%
na.omit() %>%
st_as_sf(.,coords=c("LATITUDE","LONGITUDE"),crs=102685)
Rats2blocks <- read.csv("https://github.com/mafichman/DOH-RodentAbatement-Public/tree/master/notebooks/data/rats_to_blocks.csv.gz", header = TRUE)
View(Rats2blocks)
